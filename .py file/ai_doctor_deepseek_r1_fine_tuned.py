# -*- coding: utf-8 -*-
"""AI-Doctor-DeepSeek-R1-Fine Tuned.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uvl8cLJ2yPUuZVLA1PEX0pzSArYrM5Cp

**Step1:** Create and Setup Hugging Face API Access Token Key

**Step2:** Install required dependencies
"""

# Step2: Install required dependencies
!pip install "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"
!pip install trl==0.14.0 peft==0.14.0 xformers==0.0.28.post3
!pip install torch==2.5.1 torchvision==0.20.1 --index-url https://download.pytorch.org/whl/cu124

from unsloth import FastLanguageModel
import torch
from trl import SFTTrainer
from unsloth import is_bfloat16_supported
from huggingface_hub import notebook_login, login
from transformers import TrainingArguments
from datasets import load_dataset
import wandb

from google.colab import userdata
hf_token = userdata.get('HF_TOKEN2')
login(hf_token)

# Check GPU availability
import torch

print("Device Name: ", torch.cuda.get_device_name(0) if torch.cuda.is_available() else "CUDA is not available." )

"""**Step:** Setup a pretrained DeepSeek-R1"""

model_name = "deepseek-ai/DeepSeek-R1-Distill-Llama-8B"
max_sequence_length = 2048
dtype = None
load_in_4bit = True

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = model_name,
    max_seq_length=max_sequence_length,
    dtype=dtype,
    load_in_4bit=load_in_4bit,
    token = hf_token
)

train_prompt_style = """
You are a friendly and helpful medical assistant chatbot. Provide clear and simple explanations about common health questions and symptoms. Always remind users that your advice is general and does not replace a visit to a doctor. Encourage users to seek professional medical help for serious or urgent issues.

### Query:
{}

### Answer:
<think>{}
"""

inputs = tokenizer([train_prompt_style.format(question, "")], return_tensors="pt").to("cuda")

# Run Inference on the model

# Define a test question
question = """What could be the cause of a persistent cough lasting more than two weeks?"""

# Tokenize the Input
inputs = tokenizer([train_prompt_style.format(question, "", "")], return_tensors="pt").to("cuda")

# Generate a response
outputs = model.generate(
    input_ids = inputs.input_ids,
    attention_mask = inputs.attention_mask,
    max_new_tokens = 1200,
    use_cache = True,
    # eos_token_id = tokenizer.eos_token_id,
    # pad_token_id = tokenizer.eos_token_id,
)


# Decode the response tokens back to text

response = tokenizer.batch_decode(outputs)
print(response)

medical_dataset = load_dataset("FreedomIntelligence/medical-o1-reasoning-SFT", "en", split= "train[:500]", trust_remote_code = True)

# medical_dataset[0]

EOS_TOKEN = tokenizer.eos_token
EOS_TOKEN

#prepare fint-tuning
def preprocess_input_data(examples):
  inputs = examples["Question"]
  cot = examples["Complex_CoT"]
  outputs = examples["Response"]

  texts = []
  for input, cot, output in zip(inputs, cot, outputs):
    text = train_prompt_style.format(input, cot, outputs) + EOS_TOKEN
    texts.append(text)


  return {"text": texts}

finetune_dataset = medical_dataset.map(preprocess_input_data, batched=True)

finetune_dataset["text"][0]

model_lora = FastLanguageModel.get_peft_model(
    model = model,
    r = 16,
    target_modules = [
        "q_proj",
        "k_proj",
        "v_proj",
        "o_proj",
        "gate_proj",
        "up_proj",
        "down_proj"
    ],
    lora_alpha = 16,
    lora_dropout = 0,
    bias = "none",
    use_gradient_checkpointing = "unsloth",
    random_state = 3047,
    use_rslora = False,
    loftq_config = None
)

if hasattr(model, 'unwrapped_old_generate'):
  del model.unwrapped_old_generate

trainer = SFTTrainer(
  model = model_lora,
  tokenizer = tokenizer,
  train_dataset = finetune_dataset,
  dataset_text_field = "text",
  max_seq_length = max_sequence_length,
  dataset_num_proc = 1,


  # Define training args
  args = TrainingArguments(
    per_device_train_batch_size = 2,
    gradient_accumulation_steps = 4,
    warmup_steps = 5,
    num_train_epochs = 1,
    max_steps = 60,
    learning_rate = 2e-4,
    fp16 = not is_bfloat16_supported(),
    bf16=is_bfloat16_supported(),
    logging_steps = 10,
    optim = "adamw_8bit",
    weight_decay = 0.01,
    lr_scheduler_type="linear",
    seed=3407,
    output_dir = "outputs",
  ),
)

# Setup WANDB
from google.colab import userdata
wnb_token = userdata.get("WANDB_API_TOKEN")
# Login to WnB
import wandb
wandb.login(key=wnb_token)
run = wandb.init(
    project='Fine-tune-DeepSeek-R1-on-Medical-CoT-Dataset',
    job_type="training",
    anonymous="allow"
)



# Start training the model
trainer.train()

wandb.finish()

# Run Inference on the model

# Define a test question
question = """What could be the cause of a persistent cough lasting more than two weeks?"""
FastLanguageModel.for_inference(model_lora)
# Tokenize the Input
inputs = tokenizer([train_prompt_style.format(question, "")], return_tensors="pt").to("cuda")

# Generate a response
outputs = model_lora.generate(
    input_ids = inputs.input_ids,
    attention_mask = inputs.attention_mask,
    max_new_tokens = 1200,
    use_cache = True,
    # eos_token_id = tokenizer.eos_token_id,
    # pad_token_id = tokenizer.eos_token_id,
)


# Decode the response tokens back to text

response = tokenizer.batch_decode(outputs)
print(response[0].split("### Answer:")[1])

print(response)